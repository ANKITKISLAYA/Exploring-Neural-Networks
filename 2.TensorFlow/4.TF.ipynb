{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "https://chromium.googlesource.com/external/github.com/tensorflow/tensorflow/+/r0.7/tensorflow/g3doc/tutorials/mnist/beginners/index.md\n",
    "\n",
    "Flattening the data throws away information about the 2D structure of the image. Isn‘t that bad? Well, the best computer vision methods do exploit this structure, and we will in later tutorials. But the simple method we will be using here, a softmax regression, won’t.\n",
    "\n",
    "The result is that mnist.train.images is a tensor (an n-dimensional array) with a shape of [55000, 784]. The first dimension indexes the images and the second dimension indexes the pixels in each image. Each entry in the tensor is the pixel intensity between 0 and 1, for a particular pixel in a particular image.\n",
    "\n",
    "The corresponding labels in MNIST are numbers between 0 and 9, describing which digit a given image is of. For the purposes of this tutorial, we're going to want our labels as “one-hot vectors”. A one-hot vector is a vector which is 0 in most dimensions, and 1 in a single dimension. In this case, the \\(n\\)th digit will be represented as a vector which is 1 in the \\(n\\)th dimensions. For example, 3 would be \\([0,0,0,1,0,0,0,0,0,0]\\). Consequently, mnist.train.labels is a [55000, 10] array of floats.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-batching\n",
    " \n",
    " In Mini batching we take a subset or a batch of data randomly and train the network weights on this batch with Gradient descent. This provides ability to train a model, even if a computer lacks memory to store entire dataset.\n",
    " Since these batches are random we are performing SDG with each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 ... 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "# Import MNIST data\n",
    "#mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "# train_features = mnist.train.images\n",
    "# test_features = mnist.test.images\n",
    "\n",
    "# train_labels = mnist.train.labels.astype(np.float32)\n",
    "# test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "(train_features, train_labels), (test_features, test_labels) = keras.datasets.mnist.load_data()\n",
    "\n",
    "y_train = np.zeros((train_labels.shape[0], train_labels.max()+1), dtype=np.float32)\n",
    "y_train[np.arange(train_labels.shape[0]), train_labels] = 1\n",
    "y_test = np.zeros((test_labels.shape[0], test_labels.max()+1), dtype=np.float32)\n",
    "y_test[np.arange(test_labels.shape[0]), test_labels] = 1\n",
    "\n",
    "\n",
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "l = np.zeros((test_labels.shape[0],test_labels.max()+1), dtype=np.float32)\n",
    "l[np.arange(test_labels.shape[0]),test_labels] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we want to create batch of 128 samples and we want to divide 1000 samples . But the divided samples will not be of equal size, it will be like 7 batches of 128 samples, and 1 batch of 104 samples. (7 * 128 + 1 * 104 = 1000).\n",
    "In that case , the sizes of batch would vary we will use Tensorflow's tf.placeholder() function to recieve batch of varying sizes.\n",
    "\n",
    "if each sample had n_input = 784 features and n_classes = 10 possible labels, the dimensions for features would be [None, n_input] and labels would be [None, n_classes].\n",
    "\n",
    "The None dimension is a placeholder for the batch size. At runtime, TensorFlow will accept any batch size greater than 0.\n",
    "Going back to our earlier example, this setup allows you to feed features and labels into the model as either the batches of 128 samples or the single batch of 104 samples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batches(batch_size, features, labels):\n",
    "    \"\"\"\n",
    "    Create batches of features and labels\n",
    "    :param batch_size: The batch size\n",
    "    :param features: List of features\n",
    "    :param labels: List of labels\n",
    "    :return: Batches of (Features, Labels)\n",
    "    \"\"\"\n",
    "    assert len(features) == len(labels)\n",
    "    outout_batches = []\n",
    "    \n",
    "    sample_size = len(features)\n",
    "    for start_i in range(0, sample_size, batch_size):\n",
    "        end_i = start_i + batch_size\n",
    "        batch = [features[start_i:end_i], labels[start_i:end_i]]\n",
    "        outout_batches.append(batch)\n",
    "        \n",
    "    return outout_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder_20:0\", shape=(?, 784), dtype=float32)\n",
      "Tensor(\"Placeholder_21:0\", shape=(?, 10), dtype=float32)\n",
      "Test Accuracy: 0.8513000011444092\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "# Import MNIST data\n",
    "#mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "# train_features = mnist.train.images\n",
    "# test_features = mnist.test.images\n",
    "\n",
    "# train_labels = mnist.train.labels.astype(np.float32)\n",
    "# test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "(train_features, train_labels), (test_features, test_labels) = keras.datasets.mnist.load_data()\n",
    "\n",
    "#one hot encoding of labels\n",
    "y_train = np.zeros((train_labels.shape[0], train_labels.max()+1), dtype=np.float32)\n",
    "y_train[np.arange(train_labels.shape[0]), train_labels] = 1\n",
    "y_test = np.zeros((test_labels.shape[0], test_labels.max()+1), dtype=np.float32)\n",
    "y_test[np.arange(test_labels.shape[0]), test_labels] = 1\n",
    "\n",
    "train_labels = y_train\n",
    "test_labels = y_test\n",
    "\n",
    "train_features = train_features.reshape(60000,784)\n",
    "test_features = test_features.reshape(10000,784)\n",
    "\n",
    "learning_rate = 0.001\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "print(features)\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "print(labels)\n",
    "\n",
    "# Weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "# Logits - xW + b\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "# TODO: Set batch size\n",
    "batch_size = 50\n",
    "assert batch_size is not None, 'You must set the batch size'\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # TODO: Train optimizer on all batches\n",
    "    for batch_features, batch_labels in batches(batch_size, train_features, train_labels):\n",
    "        # for batch_features, batch_labels in ______\n",
    "        sess.run(optimizer, feed_dict={features: batch_features, labels: batch_labels})\n",
    "\n",
    "    # Calculate accuracy for test dataset\n",
    "    test_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: test_features, labels: test_labels})\n",
    "\n",
    "print('Test Accuracy: {}'.format(test_accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
