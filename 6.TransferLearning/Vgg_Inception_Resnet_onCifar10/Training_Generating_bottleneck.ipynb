{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d38fa76e-04a4-4b8a-8f28-2a41f935f186",
   "metadata": {},
   "source": [
    "### Training the weights and then generating Bottleneck Feature generation for sample cifar10 and traffic dataset using Vgg, Resnet and Inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5dc1318-c641-49b7-8300-0c269cb0f16c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.8'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.version.VERSION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c617933-8bef-496a-9de8-bf9d046c64df",
   "metadata": {},
   "source": [
    "#### Exploring cifar10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d6a15b1-dca6-4aed-942b-8e12526a3c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.datasets import cifar10\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "(X_train, y_train), (_, _) = cifar10.load_data()\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a261c89-9c3a-490e-a212-f9250db05d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 32, 32, 3)\n",
      "(40000, 1)\n",
      "(10000, 32, 32, 3)\n",
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52d3596-a5d4-49f1-86de-01b1b84ac626",
   "metadata": {},
   "source": [
    "#### Checking GPU Access\n",
    "\n",
    "Here we have installed tensorflow-directml to access our AMD radeon graphics\n",
    "\n",
    "TensorFlow with DirectML enables training and inference of complex machine learning models on a wide range of DirectX 12-compatible hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24254e46-bf15-4057-b636-5efd749ad046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:DML:0', device_type='DML')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.experimental.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c75da92-3a93-4895-9b30-62706f477d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc80957-fb36-4372-8d3e-61a0843463a7",
   "metadata": {},
   "source": [
    "#### Importing Necessary Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "86460cb5-d693-4db6-b9c7-4c1adeaaf27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import Input, AveragePooling2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model\n",
    "from keras.datasets import cifar10\n",
    "import pickle\n",
    "#import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models, losses\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.activations import softmax\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51e7b23-9059-42c9-b537-ec5868b80fcb",
   "metadata": {},
   "source": [
    "#### Defining the dataset to train, network to choose and set batch_size and height, width and channel of input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f58688ae-70d3-4b04-91e7-60389fe174f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make bottleneck features for either cifar10 or traffic dataset. \n",
    "dataset = 'cifar10'\n",
    "\n",
    "# he model to bottleneck, ex. 'vgg', 'inception', or 'resnet\n",
    "network = 'vgg'\n",
    "\n",
    "# The batch size for the generator\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "h, w, ch = 224, 224, 3\n",
    "if network == 'inception':\n",
    "    h, w, ch = 299, 299, 3\n",
    "    from keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "img_placeholder = tf.placeholder(\"uint8\", (None, 32, 32, 3))\n",
    "resize_op = tf.image.resize_images(img_placeholder, (h, w), method=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735b2398-8fab-4442-907b-715c1b1de1c1",
   "metadata": {},
   "source": [
    "#### Defining our generator Function to yield our batches of our Sample and batches of our labels\n",
    "\n",
    "generator takes a session, data, labels, and batch size as input and generates batches of preprocessed data and corresponding labels using TensorFlow operations. The generator is used to feed the data into the model during the prediction phase.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fa60290d-251c-4fec-b649-7c8fac57a04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gen(session, data, labels, batch_size):\n",
    "    def _f():\n",
    "        start = 0\n",
    "        end = start + batch_size\n",
    "        n = data.shape[0]\n",
    " \n",
    "        while True:\n",
    "            X_batch = session.run(resize_op, {img_placeholder: data[start:end]})\n",
    "            X_batch = preprocess_input(X_batch)\n",
    "            y_batch = labels[start:end]\n",
    "            \n",
    "            # One-hot encode the labels\n",
    "            y_batch = to_categorical(y_batch, num_classes=10)\n",
    "            \n",
    "            start += batch_size\n",
    "            end += batch_size\n",
    "            if start >= n:\n",
    "                start = 0\n",
    "                end = batch_size\n",
    "\n",
    "            print(start, end)\n",
    "            yield (X_batch, y_batch)\n",
    "\n",
    "    return _f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ddc89e-e517-41b6-8ab4-70d4b7ceba46",
   "metadata": {},
   "source": [
    "#### Creating model\n",
    "\n",
    "creates the selected CNN model (ResNet50, VGG16, or InceptionV3) and returns the model instance.\n",
    "\n",
    "Modified our Vgg to train it on our cifar10 dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b0f67a18-b610-4dfc-9a46-239cef7783fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    input_tensor = Input(shape=(h, w, ch))\n",
    "    if network == 'vgg':\n",
    "        model = VGG16(input_tensor=input_tensor, include_top=True)\n",
    "        x = model.output\n",
    "        x = Dense(10, activation='softmax')(x)\n",
    "        model = Model(model.input, x)\n",
    "    elif network == 'inception':\n",
    "        model = InceptionV3(input_tensor=input_tensor, include_top=False)\n",
    "        x = model.output\n",
    "        x = AveragePooling2D((8, 8), strides=(8, 8))(x)\n",
    "        model = Model(model.input, x)\n",
    "    else:\n",
    "        model = ResNet50(input_tensor=input_tensor, include_top=False)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312166aa-7d8c-4ebb-9f7d-e8c9b4b81eea",
   "metadata": {},
   "source": [
    "#### Loading the dataset selected either cifar10 or traffic signs and generating bottleneck features and training the weights \n",
    "\n",
    "loading the dataset (either CIFAR10 or a traffic signs) and splits it into training and validation sets\n",
    "\n",
    "defining the output file names for saving the bottleneck features.\n",
    "\n",
    "creating a TensorFlow session and sets it as the default session.\n",
    "\n",
    "Training our Vgg network on cifar10 dataset\n",
    "\n",
    "For the training dataset and validation dataset, the model.predict_generator method is called, passing the generator function and the number of samples in the training dataset. The resulting bottleneck features are saved along with the corresponding labels in a pickle file.\n",
    "\n",
    "Steps i have taken 313 for training and 79 for validation using Steps = len(X_train)/batch_size in order to cover whole dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7bf3512b-df2a-4454-a278-06b769f84be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing to (224, 224, 3)\n",
      "Saving to ...\n",
      "vgg_cifar10_bottleneck_features_train.p\n",
      "vgg_cifar10_bottleneck_features_validation.p\n",
      "model.output:- Tensor(\"dense_5/Softmax:0\", shape=(?, 10), dtype=float32)\n",
      "Bottleneck training\n",
      "Epoch 1/1\n",
      "16 32\n",
      "32 48\n",
      "48 64\n",
      "64 80\n",
      "80 96\n",
      "96 112\n",
      "112 128\n",
      "128 144\n",
      "144 160\n",
      "160 176\n",
      "176 192\n",
      "192 208\n",
      " 1/62 [..............................] - ETA: 6:45 - loss: 2.3007 - accuracy: 0.1250208 224\n",
      " 2/62 [..............................] - ETA: 5:20 - loss: 2.3055 - accuracy: 0.0938224 240\n",
      " 3/62 [>.............................] - ETA: 4:48 - loss: 2.3005 - accuracy: 0.1042240 256\n",
      " 4/62 [>.............................] - ETA: 4:31 - loss: 2.3041 - accuracy: 0.0781256 272\n",
      " 5/62 [=>............................] - ETA: 4:20 - loss: 2.3041 - accuracy: 0.0875272 288\n",
      " 6/62 [=>............................] - ETA: 4:10 - loss: 2.3045 - accuracy: 0.0729288 304\n",
      " 7/62 [==>...........................] - ETA: 4:03 - loss: 2.3053 - accuracy: 0.0804304 320\n",
      " 8/62 [==>...........................] - ETA: 3:57 - loss: 2.3056 - accuracy: 0.0781320 336\n",
      " 9/62 [===>..........................] - ETA: 3:51 - loss: 2.3072 - accuracy: 0.0764336 352\n",
      "10/62 [===>..........................] - ETA: 3:45 - loss: 2.3050 - accuracy: 0.0813352 368\n",
      "11/62 [====>.........................] - ETA: 3:39 - loss: 2.3050 - accuracy: 0.0909368 384\n",
      "12/62 [====>.........................] - ETA: 3:34 - loss: 2.3041 - accuracy: 0.0937384 400\n",
      "13/62 [=====>........................] - ETA: 3:29 - loss: 2.3061 - accuracy: 0.0913400 416\n",
      "14/62 [=====>........................] - ETA: 3:23 - loss: 2.3066 - accuracy: 0.0937416 432\n",
      "15/62 [======>.......................] - ETA: 3:19 - loss: 2.3050 - accuracy: 0.1042432 448\n",
      "16/62 [======>.......................] - ETA: 3:14 - loss: 2.3037 - accuracy: 0.1055448 464\n",
      "17/62 [=======>......................] - ETA: 3:10 - loss: 2.3036 - accuracy: 0.1066464 480\n",
      "18/62 [=======>......................] - ETA: 3:05 - loss: 2.3030 - accuracy: 0.1042480 496\n",
      "19/62 [========>.....................] - ETA: 3:00 - loss: 2.3030 - accuracy: 0.0987496 512\n",
      "20/62 [========>.....................] - ETA: 2:56 - loss: 2.3026 - accuracy: 0.1000512 528\n",
      "21/62 [=========>....................] - ETA: 2:52 - loss: 2.3021 - accuracy: 0.1012528 544\n",
      "22/62 [=========>....................] - ETA: 2:47 - loss: 2.3014 - accuracy: 0.0994544 560\n",
      "23/62 [==========>...................] - ETA: 2:43 - loss: 2.3013 - accuracy: 0.0978560 576\n",
      "24/62 [==========>...................] - ETA: 2:39 - loss: 2.3023 - accuracy: 0.0937576 592\n",
      "25/62 [===========>..................] - ETA: 2:35 - loss: 2.3020 - accuracy: 0.0900592 608\n",
      "26/62 [===========>..................] - ETA: 2:30 - loss: 2.3017 - accuracy: 0.0865608 624\n",
      "27/62 [============>.................] - ETA: 2:26 - loss: 2.3018 - accuracy: 0.0833624 640\n",
      "28/62 [============>.................] - ETA: 2:21 - loss: 2.3019 - accuracy: 0.0804640 656\n",
      "29/62 [=============>................] - ETA: 2:17 - loss: 2.3014 - accuracy: 0.0841656 672\n",
      "30/62 [=============>................] - ETA: 2:13 - loss: 2.3017 - accuracy: 0.0812672 688\n",
      "31/62 [==============>...............] - ETA: 2:09 - loss: 2.3016 - accuracy: 0.0827688 704\n",
      "32/62 [==============>...............] - ETA: 2:05 - loss: 2.3014 - accuracy: 0.0859704 720\n",
      "33/62 [==============>...............] - ETA: 2:00 - loss: 2.3015 - accuracy: 0.0833720 736\n",
      "34/62 [===============>..............] - ETA: 1:56 - loss: 2.3015 - accuracy: 0.0846736 752\n",
      "35/62 [===============>..............] - ETA: 1:52 - loss: 2.3011 - accuracy: 0.0839752 768\n",
      "36/62 [================>.............] - ETA: 1:48 - loss: 2.3006 - accuracy: 0.0851768 784\n",
      "37/62 [================>.............] - ETA: 1:44 - loss: 2.3008 - accuracy: 0.0845784 800\n",
      "38/62 [=================>............] - ETA: 1:39 - loss: 2.3008 - accuracy: 0.0839800 816\n",
      "39/62 [=================>............] - ETA: 1:35 - loss: 2.3005 - accuracy: 0.0817816 832\n",
      "40/62 [==================>...........] - ETA: 1:31 - loss: 2.3002 - accuracy: 0.0813832 848\n",
      "41/62 [==================>...........] - ETA: 1:27 - loss: 2.3002 - accuracy: 0.0808848 864\n",
      "42/62 [===================>..........] - ETA: 1:23 - loss: 2.3000 - accuracy: 0.0804864 880\n",
      "43/62 [===================>..........] - ETA: 1:18 - loss: 2.3001 - accuracy: 0.0785880 896\n",
      "44/62 [====================>.........] - ETA: 1:14 - loss: 2.3005 - accuracy: 0.0781896 912\n",
      "45/62 [====================>.........] - ETA: 1:10 - loss: 2.3005 - accuracy: 0.0819912 928\n",
      "46/62 [=====================>........] - ETA: 1:06 - loss: 2.3008 - accuracy: 0.0829928 944\n",
      "47/62 [=====================>........] - ETA: 1:02 - loss: 2.3006 - accuracy: 0.0851944 960\n",
      "48/62 [======================>.......] - ETA: 58s - loss: 2.3002 - accuracy: 0.0885 960 976\n",
      "49/62 [======================>.......] - ETA: 53s - loss: 2.2999 - accuracy: 0.0880976 992\n",
      "50/62 [=======================>......] - ETA: 49s - loss: 2.2999 - accuracy: 0.0925992 1008\n",
      "51/62 [=======================>......] - ETA: 45s - loss: 2.2997 - accuracy: 0.09190 16\n",
      "52/62 [========================>.....] - ETA: 41s - loss: 2.3004 - accuracy: 0.091316 32\n",
      "53/62 [========================>.....] - ETA: 37s - loss: 2.3010 - accuracy: 0.090832 48\n",
      "54/62 [=========================>....] - ETA: 33s - loss: 2.3016 - accuracy: 0.091448 64\n",
      "55/62 [=========================>....] - ETA: 28s - loss: 2.3012 - accuracy: 0.092064 80\n",
      "56/62 [==========================>...] - ETA: 24s - loss: 2.3012 - accuracy: 0.094980 96\n",
      "57/62 [==========================>...] - ETA: 20s - loss: 2.3007 - accuracy: 0.094396 112\n",
      "58/62 [===========================>..] - ETA: 16s - loss: 2.3008 - accuracy: 0.0938112 128\n",
      "59/62 [===========================>..] - ETA: 12s - loss: 2.3006 - accuracy: 0.0953128 144\n",
      "60/62 [============================>.] - ETA: 8s - loss: 2.3007 - accuracy: 0.0937 144 160\n",
      "61/62 [============================>.] - ETA: 4s - loss: 2.3006 - accuracy: 0.0932160 176\n",
      "62/62 [==============================] - 257s 4s/step - loss: 2.3009 - accuracy: 0.0927\n",
      "16 32\n",
      "32 48\n",
      "48 64\n",
      "64 80\n",
      "80 96\n",
      "96 112\n",
      "112 128\n",
      "128 144\n",
      "144 160\n",
      "160 176\n",
      "176 192\n",
      "192 208\n",
      "208 224\n",
      "Bottleneck validation\n",
      "16 32\n",
      "32 48\n",
      "48 64\n",
      "64 80\n",
      "80 96\n",
      "96 112\n",
      "0 16\n",
      "16 32\n",
      "32 48\n",
      "48 64\n",
      "64 80\n",
      "80 96\n",
      "96 112\n"
     ]
    }
   ],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "if dataset == 'cifar10':\n",
    "    (X_train, y_train), (_, _) = cifar10.load_data()\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "else:\n",
    "    with open('data/train.p', mode='rb') as f:\n",
    "        train = pickle.load(f)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(train['features'], train['labels'], test_size=0.33, random_state=0)\n",
    "\n",
    "train_output_file = \"{}_{}_{}.p\".format(network, dataset, 'bottleneck_features_train')\n",
    "validation_output_file = \"{}_{}_{}.p\".format(network, dataset, 'bottleneck_features_validation')\n",
    "\n",
    "print(\"Resizing to\", (w, h, ch))\n",
    "print(\"Saving to ...\")\n",
    "print(train_output_file)\n",
    "print(validation_output_file)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    K.set_session(sess)\n",
    "    K.set_learning_phase(1)\n",
    "\n",
    "    model = create_model()\n",
    "    \n",
    "    model.compile(optimizer='adam', loss=losses.categorical_crossentropy, metrics= ['accuracy'])\n",
    "    \n",
    "    # Running the training on a small sample of data (due to limited computational resources) \n",
    "    # in order to check our code\n",
    "    X_train = X_train[:1000]\n",
    "    y_train = y_train[:1000]\n",
    "    X_val = X_val[:100]\n",
    "    y_val = y_val[:100]\n",
    "\n",
    "    print('Bottleneck training')\n",
    "    train_gen = gen(sess, X_train, y_train, batch_size)\n",
    "    model.fit_generator(train_gen(), steps_per_epoch=len(X_train) // batch_size, epochs=1)\n",
    "    # predict_generator Generates predictions for the input samples from a data generator\n",
    "    bottleneck_features_train = model.predict_generator(train_gen(), steps=2)\n",
    "    data = {'features': bottleneck_features_train, 'labels': y_train}\n",
    "    pickle.dump(data, open(train_output_file, 'wb'))\n",
    "\n",
    "    print('Bottleneck validation')\n",
    "    val_gen = gen(sess, X_val, y_val, batch_size)\n",
    "    bottleneck_features_validation = model.predict_generator(val_gen(), steps=2)\n",
    "    data = {'features': bottleneck_features_validation, 'labels': y_val}\n",
    "    pickle.dump(data, open(validation_output_file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e79ef5-553c-4517-8d3f-f9fa2ce61eb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7c3dd3f-05ea-4fc0-b2f5-db6cbae21b37",
   "metadata": {},
   "source": [
    "#### Reading the pickle file generated on our sample dataset and exploring the features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "69cb5b86-4bcb-4c85-b199-73c25e5e066e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import collections, numpy as np\n",
    "\n",
    "training_file = \"vgg_cifar10_bottleneck_features_train.p\"\n",
    "validation_file = \"vgg_cifar10_bottleneck_features_validation.p\"\n",
    "\n",
    "with open(training_file, 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "with open(validation_file, 'rb') as f:\n",
    "    validation_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fcd743fb-e91c-4641-8e7b-dbb7be384f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Bottleneck features shape :- (32, 10)\n",
      "Training Bottleneck labels shape :- (1000, 1)\n",
      "Validation Bottleneck features shape :- (32, 10)\n",
      "Validation Bottleneck labels shape :- (100, 1)\n",
      "Unique Classes :- [0 1 2 3 4 5 6 7 8 9]\n",
      "Number of Examples in each Class :- Counter({7: 111, 8: 106, 4: 105, 9: 103, 3: 100, 6: 100, 0: 98, 5: 97, 1: 93, 2: 87})\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Bottleneck features shape :-\",train_data['features'].shape)\n",
    "print(\"Training Bottleneck labels shape :-\",train_data['labels'].shape)\n",
    "print(\"Validation Bottleneck features shape :-\",validation_data['features'].shape)\n",
    "print(\"Validation Bottleneck labels shape :-\",validation_data['labels'].shape)\n",
    "print(\"Unique Classes :-\", np.unique(train_data['labels']))\n",
    "labels = train_data['labels'].reshape(train_data['labels'].shape[0])\n",
    "counter = collections.Counter(labels)\n",
    "print(\"Number of Examples in each Class :-\", counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ffbb43-36a8-45a1-85ae-fae24490bd65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
