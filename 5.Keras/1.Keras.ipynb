{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Model\n",
    "\n",
    "The keras.models.Sequential class is a wrapper for the neural network model. It provides common functions like fit(), evaluate(), and compile().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "# Create the Sequential model\n",
    "model = Sequential()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers\n",
    "\n",
    "A Keras layer is just like a neural network layer. There are fully connected layers, max pool layers, and activation layers. You can add a layer to the model using the model's add() function. For example, a simple model would look like this:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "\n",
    "# Create the Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "#1st Layer - Add a flatten layer\n",
    "model.add(Flatten(input_shape=(32, 32, 3)))\n",
    "\n",
    "#2nd Layer - Add a fully connected layer\n",
    "model.add(Dense(100))\n",
    "\n",
    "#3rd Layer - Add a ReLU activation layer\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "#4th Layer - Add a fully connected layer\n",
    "model.add(Dense(60))\n",
    "\n",
    "#5th Layer - Add a ReLU activation layer\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras will automatically infer the shape of all layers after the first layer. This means you only have to set the input dimensions for the first layer.\n",
    "\n",
    "The first layer from above, model.add(Flatten(input_shape=(32, 32, 3))), sets the input dimension to (32, 32, 3) and output dimension to (3072=32 x 32 x 3). The second layer takes in the output of the first layer and sets the output dimensions to (100). This chain of passing output to the next layer continues until the last layer, which is the output of the model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "\n",
    "# Load pickled data\n",
    "import pickle\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file = \"small_train_traffic.p\"\n",
    "testing_file = \"small_test_traffic.p\"\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "#with open(validation_file, mode='rb') as f:\n",
    "#    valid = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "    \n",
    "X_train, y_train = train['features'], train['labels']\n",
    "#X_validation, y_validation = valid['features'], valid['labels']\n",
    "X_test, y_test = test['features'], test['labels']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x22b98e74f10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(X_train[5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = Sequential()\n",
    "model.add(data_augmentation)\n",
    "model.add(Rescaling(1./255))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(len(labelfile), activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Rescaling(1./255))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "\n",
    "#1st Layer - Add a flatten layer\n",
    "model.add(Flatten(input_shape=(32, 32, 3)))\n",
    "\n",
    "#2nd Layer - Add a fully connected layer\n",
    "model.add(Dense(100))\n",
    "\n",
    "#3rd Layer - Add a ReLU activation layer\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "#4th Layer - Add a fully connected layer\n",
    "model.add(Dense(5))\n",
    "\n",
    "#5th Layer - Add a ReLU activation layer\n",
    "model.add(Activation('relu'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
